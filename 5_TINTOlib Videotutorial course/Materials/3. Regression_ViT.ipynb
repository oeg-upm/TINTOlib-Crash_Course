{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EybOZ6hSjpCF"
   },
   "source": [
    "<h1><font color=\"#113D68\" size=6>TINTOlib: Converting Tidy Data into Synthetic Images</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Template Regression problem with a Vision Transformer (ViT)</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo-Cara</font><br>\n",
    "<font color=\"#113D68\" size=3>Ra√∫l Garc√≠a-Castro</font><br>\n",
    "<font color=\"#113D68\" size=3>Jiayun Liu</font><br>\n",
    "</div>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SBn5LbYlRIC"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "More information about [Manuel Castillo-Cara](https://www.manuelcastillo.eu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puedes ver m√°s cursos de Inteligencia Artificial, Machine Learning y Deep Learning con descuento en mi [P√°gina Web Personal](https://www.manuelcastillo.eu/udemy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxZCeVnqiCyi"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Licencia</font></h2>\n",
    "\n",
    "<p><small><small>Improving Deep Learning by Exploiting Synthetic Images Copyright 2024 Manuel Castillo Cara.</p>\n",
    "<p><small><small> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at </p>\n",
    "<p><small><small> <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">https://www.apache.org/licenses/LICENSE-2.0</a> </p>\n",
    "<p><small><small> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l5nFzsdjpCW"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Index</font></h2>\n",
    "\n",
    "* [0. Context](#section0)\n",
    "* [1. Description](#section1)\n",
    "    * [1.1. Main Features](#section11)\n",
    "    * [1.2. Citation](#section12)\n",
    "    * [1.3. Documentation and License](#section13)\n",
    "* [2. Libraries](#section2)\n",
    "    * [2.1. System setup](#section21)\n",
    "    * [2.2. Invoke the libraries](#section22)\n",
    "* [3. Data processing](#section3)\n",
    "    * [3.1. Read the dataset](#section31)\n",
    "    * [3.2. Create images with TINTOlib](#section32)\n",
    "    * [3.3. Generate images](#section33)\n",
    "    * [3.4. Read images](#section34)\n",
    "    * [3.5. Mix images and tidy data](#section35)\n",
    "* [4. Pre-modelling phase](#section4)\n",
    "    * [4.1. Data curation](#section41)\n",
    "* [5. Modelling with ViT](#section5)\n",
    "    * [5.1. ViT for TINTOlib images](#section51)\n",
    "    * [5.2. MLP](#section52)\n",
    "    * [5.3. Metrics](#section53)\n",
    "    * [5.4. Compile and fit](#section54)\n",
    "* [6. Results](#section6)\n",
    "    * [6.1. Train/Validation representation](#section61)\n",
    "    * [6.2. Validation/Test evaluation](#section62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxTpMExHjpCa"
   },
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Context</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlVYt3MRrl_V"
   },
   "source": [
    "This tutorial explains how to read images generated by TINTOlib and input them into a Convolutional Neural Network (CNN). Ensure that the images have already been created using TINTOlib. For instructions on how to generate images from tabular data, refer to the TINTOlib documentation on GitHub.\n",
    "\n",
    "Remember that you can set the training to be done with GPUs to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUFttTiQiCyj"
   },
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HGeOg0ziCyj"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "See the paper from [Information Fusion Journal](https://doi.org/10.1016/j.inffus.2022.10.011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3tgsO0BjpCj"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "See the paper from [SoftwareX](https://doi.org/10.1016/j.softx.2023.101391)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RKBgDwzjpCl"
   },
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpU7pi6yjpCn"
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 1. Description</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL9RoFkEjpCq"
   },
   "source": [
    "The growing interest in the use of algorithms-based machine learning for predictive tasks has generated a large and diverse development of algorithms. However, it is widely known that not all of these algorithms are adapted to efficient solutions in certain tidy data format datasets. For this reason, novel techniques are currently being developed to convert tidy data into images with the aim of using vision models such as Convolutional Neural Network (CNN) or ViT. TINTOlib offers the opportunity to convert tidy data into images through several techniques: TINTO, IGTD, REFINED, SuperTML, BarGraph, DistanceMatrix, Combination, FeatureWrap and BIE.\n",
    "\n",
    "In this tutorial, we develop a ViT with synthetic images.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/Tabular-to-image-ViT.png\" width=\"600\" height=\"350\" alt=\"Gr√°fica\">\n",
    "  <figcaption><blockquote>ViT architecture with synthetic images.</a></blockquote></figcaption>\n",
    "</center></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFrF4C89jpCt"
   },
   "source": [
    "---\n",
    "<a id=\"section11\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 1.1. Main Features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gruE0_sjpCu"
   },
   "source": [
    "- Supports all CSV data in **[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)** format.\n",
    "- For now, the algorithm converts tabular data for binary and multi-class classification problems into machine learning.\n",
    "- Input data formats:\n",
    "    - **Tabular files**: The input data could be in **[CSV](https://en.wikipedia.org/wiki/Comma-separated_values)**, taking into account the **[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)** format.\n",
    "    - **Dataframe***: The input data could be in **[Pandas Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)**, taking into account the **[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)** format.\n",
    "    - **Tidy Data**: The **target** (variable to be predicted) should be set as the last column of the dataset. Therefore, the first columns will be the features.\n",
    "    - All data must be in numerical form. TINTOlib does not accept data in string or any other non-numeric format.\n",
    "- Runs on **Linux**, **Windows** and **macOS** systems.\n",
    "- Compatible with **[Python](https://www.python.org/)** 3.7 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At5cvb9Lirw6"
   },
   "source": [
    "---\n",
    "<a id=\"section12\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 1.2. Citation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAcloCKZirw7"
   },
   "source": [
    "**TINTOlib** is an python library that makes **Synthetic Images** from [Tidy Data](https://www.jstatsoft.org/article/view/v059i10) (also knows as **Tabular Data**).\n",
    "\n",
    "**Citing TINTO**: If you used TINTO in your work, please cite the **[SoftwareX](https://doi.org/10.1016/j.softx.2023.101391)**:\n",
    "\n",
    "```bib\n",
    "@article{softwarex_TINTO,\n",
    "    title = {TINTO: Converting Tidy Data into Image for Classification\n",
    "            with 2-Dimensional Convolutional Neural Networks},\n",
    "    journal = {SoftwareX},\n",
    "    author = {Manuel Castillo-Cara and Reewos Talla-Chumpitaz and\n",
    "              Ra√∫l Garc√≠a-Castro and Luis Orozco-Barbosa},\n",
    "    year = {2023},\n",
    "    pages = {101391},\n",
    "    issn = {2352-7110},\n",
    "    doi = {https://doi.org/10.1016/j.softx.2023.101391}\n",
    "}\n",
    "```\n",
    "\n",
    "And use-case developed in **[INFFUS Paper](https://doi.org/10.1016/j.inffus.2022.10.011)**\n",
    "\n",
    "```bib\n",
    "@article{inffus_TINTO,\n",
    "    title = {A novel deep learning approach using blurring image\n",
    "            techniques for Bluetooth-based indoor localisation},\n",
    "    journal = {Information Fusion},\n",
    "    author = {Reewos Talla-Chumpitaz and Manuel Castillo-Cara and\n",
    "              Luis Orozco-Barbosa and Ra√∫l Garc√≠a-Castro},\n",
    "    volume = {91},\n",
    "    pages = {173-186},\n",
    "    year = {2023},\n",
    "    issn = {1566-2535},\n",
    "    doi = {https://doi.org/10.1016/j.inffus.2022.10.011}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgi8NNWHirw8"
   },
   "source": [
    "---\n",
    "<a id=\"section13\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 1.3. Documentation and License</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An9470Tqirw9"
   },
   "source": [
    "TINTOlib has a wide range of documentation on both GitHub and PiPY.\n",
    "\n",
    "Moreover, TINTOlib is free and open software with Apache 2.0 license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2SJRrucirw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib in [GitHub](https://github.com/oeg-upm/TINTOlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INZnsmObiCyk"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib documentation in the official [Webpage](https://tintolib.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upfq7gH3irw_"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib documentation in [PyPI](https://tintolib.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see more information and examples in [TINTOlib Crash Course](https://github.com/oeg-upm/TINTOlib-Crash_Course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3EzYcjJjpC6"
   },
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwYF5A2njpC8"
   },
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 2. Libraries</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn-_Qzf7irxB"
   },
   "source": [
    "---\n",
    "<a id=\"section21\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 2.1. System setup</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YGCnZifirxC"
   },
   "source": [
    "Before installing the libraries you must have the `mpi4py` package installed on the native (Linux) system. This link shows how to install it:\n",
    "- Link: [`mpi4py` in Linux](https://www.geeksforgeeks.org/how-to-install-python3-mpi4py-package-on-linux/)\n",
    "\n",
    "For example, in Linux:\n",
    "\n",
    "```\n",
    "    sudo apt-get install python3\n",
    "    sudo apt install python3-pip\n",
    "    sudo apt install python3-mpi4py\n",
    "```\n",
    "\n",
    "If you are in Windows, Mac or, also, Linux, you can install from PyPI if you want:\n",
    "```\n",
    "    sudo pip3 install mpi4py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1Oro1QfirxD"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Note that you must **restart the kernel or the system** so that it can load the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsMELXdZirxE"
   },
   "source": [
    "Now, once you have installed `mpi4py` you can install the PyPI libraries and dependences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6255,
     "status": "ok",
     "timestamp": 1729271661540,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "0izeyH09iCyl",
    "outputId": "fad33e3e-9601-453b-cf7f-92eddd22b236"
   },
   "outputs": [],
   "source": [
    "!pip install -U tintolib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6928,
     "status": "ok",
     "timestamp": 1729271668465,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "tM2K4vorirxF",
    "outputId": "af536ac3-ea92-4e5d-a639-03450af506cb"
   },
   "outputs": [],
   "source": [
    "!pip install -U torchmetrics pytorch_lightning TINTOlib imblearn keras_preprocessing mpi4py tifffile tqdm seaborn bitstring opencv-python pydot graphviz keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OV3PYZxirxK"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Note that you must **restart the kernel** so that it can load the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKMOKiJXirxL"
   },
   "source": [
    "---\n",
    "<a id=\"section22\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 2.2. Invoke the libraries</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AdHKnWYsEq_"
   },
   "source": [
    "The first thing we need to do is to declare the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8311,
     "status": "ok",
     "timestamp": 1729271676773,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "PeeBbGxlpjFp",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tifffile as tifi\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras import ops\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (mean_absolute_error, mean_absolute_percentage_error,\n",
    "                             root_mean_squared_error, mean_squared_error, r2_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# TensorFlow and Keras\n",
    "from keras import layers, models, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import (Activation, BatchNormalization, concatenate,\n",
    "                                     Conv2D, Dense, Dropout, Flatten, Input,\n",
    "                                     LayerNormalization, MaxPool2D, MaxPooling2D)\n",
    "from keras.losses import MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adadelta, Adam, Adamax, SGD\n",
    "\n",
    "#Models of TINTOlib\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.combination import Combination\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.featureWrap import FeatureWrap\n",
    "from TINTOlib.bie import BIE\n",
    "\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "SEED = 64\n",
    "#torch.manual_seed(SEED)\n",
    "#torch.cuda.manual_seed(SEED)\n",
    "#torch.cuda.manual_seed_all(SEED)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwS-cKUxjpDQ"
   },
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 3. Data processing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXcRw78ljpDU"
   },
   "source": [
    "TINTOlib creates a folder structure to store images corresponding to each target in a problem. For regression problems, since there are no distinct classes, all images are stored in a single subfolder named images/. Additionally, a CSV file is generated containing:\n",
    "\n",
    "- The file paths of all images.\n",
    "- The target value for each image, which corresponds to a sample from the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 3.1. Read the dataset</font>\n",
    "\n",
    "In this part, we proceed to read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'boston'\n",
    "\n",
    "#Read CSV\n",
    "df = pd.read_csv(f\"Datasets/{dataset_name}.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the appropriate size for a square image that contains all the feature pixels, you need to calculate the square root of the total number of features. The resulting value can be used for the methods that requires inserting the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section32\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 3.2. Create images with TINTOlib</font>\n",
    "\n",
    "We prepare the declaration of the classes with the TINTOlib method we want to transform. Note that TINTOlib has several methods and we will have to choose one of them since each method generates different images.\n",
    "\n",
    "In addition, we establish the paths where the dataset is located and also the folder where the images will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "#problem_type = \"supervised\"\n",
    "problem_type = \"regression\"\n",
    "\n",
    "# Transformation methods\n",
    "#image_model = TINTO(problem=problem_type, blur=True, option='maximum', pixels=20, random_seed=SEED)\n",
    "# name = f\"TINTO_blur_maximum\"\n",
    "# image_model = REFINED(problem=problem_type, random_seed=SEED, zoom=1, n_processors=8)\n",
    "# name = f\"REFINED\"\n",
    "# image_model = IGTD(problem=problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=300, random_seed=SEED)\n",
    "# name = f\"IGTD_fEuclidean_iEuclidean_abs\"\n",
    "# image_model = BarGraph(problem=problem_type, zoom=2)\n",
    "# name = f\"BarGraph_zoom2\"\n",
    "# image_model = DistanceMatrix(problem=problem_type, zoom=2)\n",
    "# name = f\"DistanceMatrix_zoom2\"\n",
    "image_model = Combination(problem=problem_type, zoom=3)\n",
    "name = f\"Combination\"\n",
    "# image_model = SuperTML(problem=problem_type, pixels=pixel, font_size=30, feature_importance=True, random_seed=SEED)\n",
    "# name = f\"SuperTML-VF_FS30\"\n",
    "# image_model = FeatureWrap(problem = problem_type, bins=10)\n",
    "# name = f\"FeatureWrap_bins10\"\n",
    "# image_model = BIE(problem = problem_type)\n",
    "# name = f\"BIE\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "results_folder = f\"Results/{dataset_name}_{name}\"\n",
    "images_folder = f\"Synthetic_images/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib documentation in [PyPI](https://tintolib.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section33\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 3.3. Generate images</font>\n",
    "\n",
    "In this section, we generate images from the dataset using three key functions of the image generation model:\n",
    "\n",
    "- fit: Trains the image generation model without generating images. This function is used exclusively for training purposes.\n",
    "- fit_transform: Trains the image generation model and simultaneously generates images for the dataset. This function is applied to the training dataset, where the model is both trained and used to create images.\n",
    "- transform: Generates images using the pre-trained model. After training on the training dataset, this function is used to generate images for unseen data, such as validation and test datasets.\n",
    "\n",
    "Each row in the dataset is transformed into a unique image, ensuring that the number of generated images matches the number of rows in the dataset. The resulting datasets include paths to these images, which are then combined with the original data for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_val = train_test_split(df, test_size=0.20, random_state=SEED)\n",
    "X_val, X_test = train_test_split(X_val, test_size=0.50, random_state=SEED)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to streamline the repetitive process of generating images, updating paths, and combining datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(X, folder_name, generate_function, problem_type):\n",
    "    \"\"\"\n",
    "    Handles dataset processing, including image generation, path updates, \n",
    "    and combining the dataset with image paths.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        The dataset to process (training, validation, or test).\n",
    "    \n",
    "    folder_name : str\n",
    "        The name of the folder where generated images will be stored \n",
    "        (e.g., 'train', 'val', 'test').\n",
    "    \n",
    "    generate_function : function\n",
    "        The function used for training and generating images. It can be one of the following:\n",
    "        - `fit`: Trains the model without generating images.\n",
    "        - `fit_transform`: Trains the model and generates images for the dataset (used for training).\n",
    "        - `transform`: Uses the pre-trained model to generate images for validation and testing.\n",
    "    \n",
    "    problem_type : str\n",
    "        The type of problem being addressed (e.g., regression, supervised).\n",
    "        This is used to locate the corresponding `.csv` file containing image paths.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_processed : DataFrame\n",
    "        The dataset with updated image paths and raw tabular data, ready for further processing.\n",
    "    \n",
    "    y_processed : Series\n",
    "        The labels corresponding to the dataset (target values).\n",
    "    \"\"\"\n",
    "    # Generate the images if the folder does not exist\n",
    "    folder_path = f\"{images_folder}/{folder_name}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        generate_function(X, folder_path)\n",
    "    else:\n",
    "        print(f\"The images for {folder_name} are already generated\")\n",
    "\n",
    "    # Load image paths\n",
    "    img_paths = os.path.join(folder_path, f\"{problem_type}.csv\")\n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = folder_path + \"/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, X], axis=1)\n",
    "\n",
    "    # Split data and labels\n",
    "    X_processed = combined_dataset.drop(df.columns[-1], axis=1).drop(\"values\", axis=1)\n",
    "    y_processed = combined_dataset[\"values\"]\n",
    "\n",
    "    return X_processed, y_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_train\n",
    "X_train, y_train = process_dataset(X_train, \"train\", image_model.fit_transform, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_val\n",
    "X_val, y_val = process_dataset(X_val, \"val\", image_model.transform, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_test\n",
    "X_test, y_test = process_dataset(X_test, \"test\", image_model.transform, problem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF1lJWbojpD3"
   },
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 4. Pre-modelling phase</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg_Ef8MxiryD"
   },
   "source": [
    "Once the data is ready, we load it into memory with an iterator in order to pass it to the ViT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwtEi_pkiryE"
   },
   "source": [
    "---\n",
    "<a id=\"section41\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 4.1. Data curation</font>\n",
    "\n",
    "Note that each method generates images of **different pixel size**. For example:\n",
    "- `TINTO` method has a parameter that you can specify the size in pixels which by default is 20.\n",
    "- Other parameters such as `Combined` generates the size automatically and you must obtain them from the _shape_ of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F_w0k0QiryF"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib documentation in [PyPI](https://tintolib.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xOw2lzRiryI"
   },
   "source": [
    "Split in train/test/validation.\n",
    "\n",
    "Note that the partitioning of the images is also performed, in addition to the tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEJkDvi3iCyt"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "üí° **Important!!!**:  Keep in mind that, depending on the method used, you need to identify the number of pixels in the image. For example, in TINTO it is specified as a parameter, but in IGTD it is done afterwards, once the image is created (and even the pixels of width and height can be different)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2829,
     "status": "ok",
     "timestamp": 1729271720569,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "veReOdAyiryJ",
    "outputId": "8a7a8df8-c8ac-475b-a11e-0e244ceabe02"
   },
   "outputs": [],
   "source": [
    "#TIDY DATA SPLITTED\n",
    "X_train_num = X_train.drop(\"images\",axis=1)\n",
    "X_val_num = X_val.drop(\"images\",axis=1)\n",
    "X_test_num = X_test.drop(\"images\",axis=1)\n",
    "\n",
    "#IMAGES\n",
    "# For 3 channels (RGB)\n",
    "X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\n",
    "\n",
    "# For 1 channels (GRAY SCALE)\n",
    "\"\"\"X_train_img = np.array([cv2.imread(img,cv2.IMREAD_GRAYSCALE) for img in X_train[\"images\"]])\n",
    "X_val_img = np.array([cv2.imread(img,cv2.IMREAD_GRAYSCALE) for img in X_val[\"images\"]])\n",
    "X_test_img = np.array([cv2.imread(img,cv2.IMREAD_GRAYSCALE) for img in X_test[\"images\"]])\"\"\"\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale numerical data\n",
    "X_train_num = scaler.fit_transform(X_train_num)\n",
    "X_val_num = scaler.transform(X_val_num)\n",
    "X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "attributes = X_train_num.shape[1]\n",
    "height, width, channels = X_train_img[0].shape\n",
    "imgs_shape = (height, width, channels)\n",
    "\n",
    "print(\"Images shape: \",imgs_shape)\n",
    "print(\"Attributres: \",attributes)\n",
    "pixel=X_train_img[0].shape[0]\n",
    "print(\"Image size (pixels):\", pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1729271720569,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "yymPASjqiCyu",
    "outputId": "6e532688-5640-43fb-a5c6-873746a76c10"
   },
   "outputs": [],
   "source": [
    "# Plot an example image (e.g., the first image in the array)\n",
    "example_image = X_train_img[0]\n",
    "\n",
    "# Convert the image from BGR (OpenCV default) to RGB for correct color display\n",
    "example_image_rgb = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(example_image_rgb)\n",
    "plt.title(\"Example Image from X_train\")\n",
    "plt.axis('off')  # Hide the axis for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1729271720570,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "8f_NYOCXiCyu",
    "outputId": "632c1bc8-3e06-4c19-e248-172ea36813f3"
   },
   "outputs": [],
   "source": [
    "X_train_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaP_OB1yC__b"
   },
   "source": [
    "<a id=\"section5\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 5. Modeling with ViT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-bbUy0-iryN"
   },
   "source": [
    "Now we can start the ViT training. Before that we prepare the algorithm for reading data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6TMUJ0yiryj"
   },
   "source": [
    "---\n",
    "<a id=\"section52\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.1. ViT for TINTOlib images</font>\n",
    "\n",
    "This is an example of a simple ViT for TINTOlib images. Note that we are not looking for the optimization of the ViT but to show an example of TINTOlib execution.\n",
    "\n",
    "It is crucial to select an appropriate patch size. The patch size should be a divisor of the input image size; for example, an image of 20x20 with a patch size of 5 would result in a total of 16 patches (4x4 grid). Given the high computational cost, the patch size should be carefully chosen based on the dimensions of the image.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/visionTransformer.png\" width=\"500\" height=\"350\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>ViT Architecture and components. Extracted from the <a href=\"https://arxiv.org/abs/2010.11929\">ViT article</a></blockquote></figcaption>\n",
    "</center></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xia1V2F5iCyu"
   },
   "source": [
    "This code helps identify the valid patch sizes by finding the divisors of a given number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1729271720570,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "ssqVvujYiCyv"
   },
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1729271720570,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "5G9dxiSiiCyv",
    "outputId": "7e7bf82e-b942-4c47-d172-76a707fff25f"
   },
   "outputs": [],
   "source": [
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQx1g4LtiCyv"
   },
   "source": [
    "This code defines the hyperparameters for the ViT model implemented. The hyperparameters are as follows:\n",
    "\n",
    "- `image_size`: input image size.\n",
    "- `patch_size`: size of the patches extracted from the images.\n",
    "- `num_patches`: total number of patches extracted from each image.\n",
    "- `projection_dim`: dimensionality of the linear projection for the patches.\n",
    "- `num_heads`: number of attention heads in the transformer.\n",
    "- `transformer_units`: list of units in the transformer layers.\n",
    "- `transformer_layers`: number of layers in the transformer.\n",
    "- `mlp_head_units`: list of units in the dense layers of the final classifier.\n",
    "\n",
    "These hyperparameters are used to configure the ViT model and its training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729271720570,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "j34u3al1iCyv"
   },
   "outputs": [],
   "source": [
    "image_size = pixel\n",
    "patch_size = 3\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 32\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 4\n",
    "mlp_head_units = [\n",
    "    128,\n",
    "    64,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SIlEmUiiCyv"
   },
   "source": [
    "<a id=\"section511\"></a>\n",
    "## <font color=\"#004D7F\" size=5> 5.1.1. `Patches` Class</font>\n",
    "\n",
    "The `Patches` class divides an image into small, fixed-size patches, rearranging them into a tensor that can be used as input for a Vision Transformer. This is essential because Transformers work with sequences, and this class converts images into sequences of patches.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/patch_embedding.png\" width=\"850\" height=\"200\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>ViT Architecture - Patch embedding. Extracted from <a href=\"https://arxiv.org/abs/2010.11929\">Aman Arora's Blog</a></blockquote></figcaption>\n",
    "</center></figure>\n",
    "\n",
    "The `Patches` class is a subclass of `layers.Layer` in Keras, meaning it is a custom layer. This class is responsible for dividing an image into small patches that will be used as inputs to the Vision Transformer.\n",
    "\n",
    "##### `__init__` Constructor\n",
    "- `__init__` is the class constructor.\n",
    "- `patch_size` is a parameter that specifies the size of each patch into which the image will be divided.\n",
    "- `super().__init__()` calls the constructor of the base class (`layers.Layer`), initializing the layer.\n",
    "\n",
    "##### `call` Method\n",
    "- `call` is the method that defines the logic of the layer. It is invoked when a tensor (in this case, images) is passed through the layer.\n",
    "- `input_shape = ops.shape(images)` retrieves the shape (dimensions) of the input tensor `images`. Assuming `images` is a 4D tensor (batch, height, width, channels).\n",
    "- `batch_size`, `height`, `width`, and `channels` extract the respective dimensions of the image.\n",
    "- `num_patches_h` and `num_patches_w` calculate the number of patches in the height and width of the image, respectively, by dividing the corresponding dimension by `patch_size`.\n",
    "- `patches = keras.ops.image.extract_patches(images, size=self.patch_size)` uses a Keras function to extract patches of size `patch_size` from the input images.\n",
    "- `patches = ops.reshape(...)` reshapes the `patches` tensor to have the shape `(batch_size, num_patches_h * num_patches_w, patch_size * patch_size * channels)`. This means that each patch is flattened and organized into a sequence of patches.\n",
    "\n",
    "##### `get_config` Method\n",
    "\n",
    "- `get_config` is a standard method in Keras for custom layers. It allows the layer's configuration to be saved and reloaded.\n",
    "- `config = super().get_config()` calls the base class's `get_config` method to retrieve the basic configuration of the layer.\n",
    "- `config.update({\"patch_size\": self.patch_size})` adds the `patch_size` to the configuration.\n",
    "- `return config` returns the complete configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729271720570,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "wNMl-A1FiCyv"
   },
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = ops.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = ops.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNXT3M3CiCyv"
   },
   "source": [
    "<a id=\"section512\"></a>\n",
    "## <font color=\"#004D7F\" size=5> 5.1.2. Patch Encoder</font>\n",
    "\n",
    "The `PatchEncoder` class takes the image patches and projects them into a higher-dimensional space using a dense layer (`Dense`). It then adds positional information to each patch using a positional embedding layer (`Embedding`). This encoding is crucial for the functioning of Transformers, which need to know both the content of the patches and their position in the original image.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/vit.png\" width=\"450\" height=\"250\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>ViT Architecture and Transformer Encoder. Extracted from the <a href=\"https://arxiv.org/abs/2010.11929\">ViT article</a></blockquote></figcaption>\n",
    "</center></figure>\n",
    "\n",
    "The `PatchEncoder` class is a subclass of `layers.Layer` in Keras, and it is used to project and encode the image patches.\n",
    "\n",
    "##### `__init__` Constructor\n",
    "- `__init__` is the class constructor.\n",
    "- `num_patches` is the total number of patches into which the image has been divided.\n",
    "- `projection_dim` is the dimension into which the patches will be projected.\n",
    "- `self.projection` is a `Dense` layer that projects each patch into a higher-dimensional space specified by `projection_dim`.\n",
    "- `self.position_embedding` is an `Embedding` layer that creates positional embeddings for each patch, with `num_patches` as the vocabulary size and `projection_dim` as the output dimension.\n",
    "\n",
    "##### `call` Method\n",
    "- `call` is the method that defines the logic of the layer. It is invoked when a tensor (in this case, patches) is passed through the layer.\n",
    "- `positions` creates a tensor with a sequence of positions (from 0 to `num_patches - 1`), expanding the dimension to match the batch of data.\n",
    "- `projected_patches` applies a linear projection to each patch using the `Dense` layer, resulting in a higher-dimensional tensor.\n",
    "- `encoded` adds the linear projection of the patches (`projected_patches`) to the positional embeddings (`self.position_embedding(positions)`). This sum incorporates information about the position of each patch, which is crucial for the Transformer to understand the spatial arrangement of the patches.\n",
    "- `return encoded` returns the encoded tensor.\n",
    "\n",
    "##### `get_config` Method\n",
    "- `get_config` is a standard method in Keras for custom layers. It allows the layer's configuration to be saved and reloaded.\n",
    "- `config = super().get_config()` calls the base class's `get_config` method to retrieve the basic configuration of the layer.\n",
    "- `config.update({\"num_patches\": self.num_patches})` adds `num_patches` to the configuration.\n",
    "- `return config` returns the complete configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729271720570,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "777mOCgaiCyv"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EOxYaQUiCyv"
   },
   "source": [
    "<a id=\"section513\"></a>\n",
    "## <font color=\"#004D7F\" size=5> 5.1.3. Classifier</font>\n",
    "\n",
    "The `mlp` function constructs an MLP that transforms the input tensor `x` through several dense and dropout layers. Each dense layer applies a linear transformation followed by a `gelu` activation, and each dropout layer randomly deactivates a percentage of the units from the previous layer to improve model generalization. This function is useful for adding non-linear learning capacity to the model, allowing it to capture more complex relationships in the data:\n",
    "- `x`: the input tensor to be transformed.\n",
    "- `hidden_units`: a list specifying the number of units (neurons) in each hidden layer of the MLP network.\n",
    "- `dropout_rate`: the dropout rate applied after each dense layer, helping to prevent overfitting.\n",
    "\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/vit.png\" width=\"450\" height=\"250\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>ViT Architecture and Transformer Encoder. Extracted from the <a href=\"https://arxiv.org/abs/2010.11929\">ViT article</a></blockquote></figcaption>\n",
    "</center></figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729271720570,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "CyBagm7piCyw"
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ID0YHLx9iCyw"
   },
   "source": [
    "<a id=\"section514\"></a>\n",
    "## <font color=\"#004D7F\" size=5> 5.1.4. ViT Classifier</font>\n",
    "\n",
    "The `create_vit_classifier` function constructs a complete Vision Transformer classifier.\n",
    "1. It first divides the input image into patches and encodes those patches.\n",
    "2. Then, it passes the encoded patches through multiple Transformer layers, each of which includes layer normalization, multi-head attention, residual connections, and MLP layers.\n",
    "3. Finally, it normalizes and flattens the representation before passing it through a final MLP network to produce the output features.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/Encoder-decoder.png\" width=\"450\" height=\"250\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>Transformer Architecture. Extracted from the <a href=\"https://arxiv.org/abs/2010.11929\">ViT article</a></blockquote></figcaption>\n",
    "</center></figure>\n",
    "\n",
    "#### Model Input\n",
    "- `inputs`: defines the model input with a shape specified by `input_shape`.\n",
    "\n",
    "#### Patch Creation\n",
    "- `patches`: instantiates the `Patches` layer (defined earlier) to divide the input image into smaller patches of the size specified by `patch_size`.\n",
    "\n",
    "#### Patch Encoding\n",
    "- `encoded_patches`: instantiates the `PatchEncoder` layer (defined earlier) to project the patches into a high-dimensional space and add positional embeddings.\n",
    "\n",
    "#### Transformer Blocks\n",
    "- Loops through `transformer_layers` to create multiple layers of the Transformer block.\n",
    "    - `x1`: applies layer normalization to the `encoded_patches`.\n",
    "    - `attention_output`: applies a multi-head attention layer.\n",
    "    - `x2`: performs a residual (skip) connection by adding `attention_output` and `encoded_patches`.\n",
    "    - `x3`: applies another layer normalization to `x2`.\n",
    "    - `x3`: passes through an MLP using the `mlp` function defined earlier.\n",
    "    - `encoded_patches`: performs another residual connection by adding `x3` and `x2`.\n",
    "\n",
    "#### Final Representation\n",
    "- `representation`: applies layer normalization, flattens the tensor, and applies dropout for regularization.\n",
    "\n",
    "#### Final MLP Network\n",
    "- `features`: applies another MLP to the final representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729271720570,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "g80qMV4eiryk"
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = keras.Input(shape=imgs_shape)\n",
    "    # Augment data.\n",
    "    #augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=representation)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2650,
     "status": "ok",
     "timestamp": 1729271723215,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "ACvJW4mUiCyw"
   },
   "outputs": [],
   "source": [
    "vit_model = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_6M5Qjvirym"
   },
   "source": [
    "---\n",
    "<a id=\"section52\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.2. MLP</font>\n",
    "\n",
    "Finally, we pass the output to a MLP to perform a regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729271723216,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "NmeIuBTPiCyw"
   },
   "outputs": [],
   "source": [
    "output = vit_model.output\n",
    "x = Dense(128, activation=\"relu\")(output)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "model = Model(inputs=[vit_model.input], outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Qqj7Mlciryo"
   },
   "source": [
    "---\n",
    "<a id=\"section54\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.3. Metrics</font>\n",
    "\n",
    "Define metrics and some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729271723216,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "qmVREpu0DAAS"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res = K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    r2 = 1 - SS_res / (SS_tot + K.epsilon())\n",
    "    return r2\n",
    "\n",
    "METRICS = [\n",
    "    tf.keras.metrics.MeanSquaredError(name = 'mse'),\n",
    "    tf.keras.metrics.MeanAbsoluteError(name = 'mae'),\n",
    "    tf.keras.metrics.RootMeanSquaredError(name = 'rmse'),\n",
    "    r_square,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjknIMyqiryr"
   },
   "source": [
    "Print the hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 3110,
     "status": "ok",
     "timestamp": 1729271726321,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "wHBsjNmJirys",
    "outputId": "42e5dfe5-ca68-4ca6-e119-2e44a852c5e8"
   },
   "outputs": [],
   "source": [
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# Redirect the summary output to the specified file\n",
    "with open(results_folder+\"/model_summary.txt\", \"w\") as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Desactivar la visualizaci√≥n autom√°tica de matplotlib\n",
    "plt.ioff()\n",
    "# Now, you can also save the model plot\n",
    "plot_model(model, to_file=results_folder+'model_plot.png', show_shapes=True, expand_nested=True)\n",
    "# Reactivar la visualizaci√≥n autom√°tica de matplotlib (opcional)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPGJMv8Jiryz"
   },
   "source": [
    "---\n",
    "<a id=\"section55\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.5. Compile and fit</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729271726322,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "-mUXAIeZDAAT"
   },
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729271726322,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "bpK2urdRiry1"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=opt,\n",
    "    metrics = METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729271726322,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "u9qhchaFiCyx"
   },
   "outputs": [],
   "source": [
    "# Configure EarlyStopping for binary classification\n",
    "early_stopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation accuracy\n",
    "    min_delta=0.001,         # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,             # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,               # Log when training stops\n",
    "    mode='min',              # Maximize the accuracy; min the loss\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49656,
     "status": "ok",
     "timestamp": 1729271996021,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "N4dcwqkViry2",
    "outputId": "b1e4786e-75d3-44f1-a575-741f72bd30a8"
   },
   "outputs": [],
   "source": [
    "model_history=model.fit(\n",
    "    x=[X_train_img], y=y_train,\n",
    "    validation_data=([X_val_img], y_val),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks = [early_stopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729271996021,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "BcGghwkIDAAW",
    "outputId": "cd0e48c1-d702-486b-edbe-7cf5adc6d5cf"
   },
   "outputs": [],
   "source": [
    "print(model_history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMRV4Ee1DAAX"
   },
   "source": [
    "<a id=\"section6\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 6. Results</font>\n",
    "\n",
    "Finally, we can evaluate our hybrid model with the images created by TINTOlib in any of the ways represented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OsKjwHpirzI"
   },
   "source": [
    "---\n",
    "<a id=\"section61\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 6.1. Train/Validation representation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1729271996618,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "H5UXDjQwDAAY",
    "outputId": "7434e154-4eff-4959-8585-58fc52c114b8"
   },
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'], color = 'red', label = 'loss')\n",
    "plt.plot(model_history.history['val_loss'], color = 'green', label = 'val loss')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729271996619,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "X1ophmZYDAAZ",
    "outputId": "bf156dbc-fa1a-41da-dab1-80bbc1219999"
   },
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['mse'], color = 'red', label = 'mse')\n",
    "plt.plot(model_history.history['val_mse'], color = 'green', label = 'val mse')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMXf45ZyirzU"
   },
   "source": [
    "---\n",
    "<a id=\"section62\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 6.2. Validation/Test evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2396,
     "status": "ok",
     "timestamp": 1729271999009,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "MfmyHhtTirzV",
    "outputId": "2c39d67e-c499-4fd3-f48e-ec9e22a30aeb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_test= model.evaluate([X_val_img], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4213,
     "status": "ok",
     "timestamp": 1729272003215,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "ItKAjj6wDAAb",
    "outputId": "dbc4b5d1-1733-40bd-89f3-7c120e6d3e36"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict([X_test_img])\n",
    "\n",
    "test_mape = mean_absolute_percentage_error(y_test, prediction)\n",
    "test_mae = mean_absolute_error(y_test, prediction)\n",
    "test_mse = mean_squared_error(y_test, prediction)\n",
    "test_rmse = root_mean_squared_error(y_test, prediction)\n",
    "test_r2 = r2_score(y_test, prediction)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Absolute Percentage Error:\", test_mape)\n",
    "print(\"Mean Absolute Error:\", test_mae)\n",
    "print(\"Mean Squared Error:\", test_mse)\n",
    "print(\"Root Mean Squared Error:\", test_rmse)\n",
    "print(\"R2 Score:\", test_r2)\n",
    "\n",
    "# Define the metrics and their values\n",
    "metrics = {\n",
    "    \"Mean Absolute Percentage Error\": test_mape,\n",
    "    \"Mean Absolute Error\": test_mae,\n",
    "    \"Mean Squared Error\": test_mse,\n",
    "    \"Root Mean Squared Error\": test_rmse,\n",
    "    \"R2 Score\": test_r2,\n",
    "}\n",
    "\n",
    "# Save the metrics to a text file\n",
    "with open(f\"{results_folder}/metrics.txt\", \"w\") as file:\n",
    "    for metric, value in metrics.items():\n",
    "        file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "print(f\"Metrics saved to {results_folder}/metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729272003215,
     "user": {
      "displayName": "Jiayun Liu",
      "userId": "16866032985979885782"
     },
     "user_tz": -120
    },
    "id": "qmgClcfWirzk",
    "outputId": "076e8719-b25b-4703-f85d-2d510e6192ff"
   },
   "outputs": [],
   "source": [
    "train_mse = model_history.history[\"mse\"][-1]\n",
    "train_r2 = model_history.history[\"r_square\"][-1]\n",
    "\n",
    "val_mse = model_history.history[\"val_mse\"][-1]\n",
    "val_r2 = model_history.history[\"val_r_square\"][-1]\n",
    "\n",
    "print(\"Train Mean Squared Error:\", train_mse)\n",
    "print(\"Train R2 Score:\", train_r2)\n",
    "\n",
    "print(\"Val Mean Squared Error:\", val_mse)\n",
    "print(\"Val R2 Score:\", val_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ciBS76mpjGU"
   },
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
