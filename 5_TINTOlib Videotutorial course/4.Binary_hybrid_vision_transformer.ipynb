{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EybOZ6hSjpCF"
   },
   "source": [
    "<h1><font color=\"#113D68\" size=6>TINTOlib: Converting Tidy Data into Image for Hybrid Neural Networks (HyNN)</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Template Binary Classification Machine Learning problem with a Hybrid Vision Transformer (ViT+MLP)</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo-Cara</font><br>\n",
    "<font color=\"#113D68\" size=3>Ra√∫l Garc√≠a-Castro</font><br>\n",
    "<font color=\"#113D68\" size=3>Jiayun Liu</font><br>\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "More information about [Manuel Castillo-Cara](https://www.manuelcastillo.eu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puedes ver m√°s cursos de Inteligencia Artificial, Machine Learning y Deep Learning en mi [p√°gina web](https://www.manuelcastillo.eu/udemy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Licencia</font></h2>\n",
    "\n",
    "<p><small><small>Improving Deep Learning by Exploiting Synthetic Images Copyright 2024 Manuel Castillo Cara.</p>\n",
    "<p><small><small> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at </p>\n",
    "<p><small><small> <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">https://www.apache.org/licenses/LICENSE-2.0</a> </p>\n",
    "<p><small><small> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l5nFzsdjpCW"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Index</font></h2>\n",
    "\n",
    "* [0. Context](#section0)\n",
    "* [1. Description](#section1)\n",
    "    * [1.1. Main Features](#section11)\n",
    "    * [1.2. Citation](#section12)\n",
    "    * [1.3. Documentation and License](#section13)\n",
    "* [2. Libraries](#section2)\n",
    "    * [2.1. System setup](#section21)\n",
    "    * [2.2. Invoke the libraries](#section22)\n",
    "* [3. Data processing](#section3)\n",
    "    * [3.1. TINTOlib methods](#section31)\n",
    "    * [3.2. Read the dataset](#section32)\n",
    "    * [3.3. Generate images](#section33)\n",
    "    * [3.4. Read images](#section34)\n",
    "    * [3.5. Mix images and tidy data](#section35)\n",
    "* [4. Pre-modelling phase](#section4)\n",
    "    * [4.1. Data curation](#section41)\n",
    "    * [4.2. One-hot encoding](#section42)\n",
    "* [5. Modelling hybrid network](#section5)\n",
    "    * [5.1. FFNN for tabular data](#section51)\n",
    "    * [5.2. CNN for TINTOlib images](#section52)\n",
    "    * [5.3. Concatenate branches](#section53)\n",
    "    * [5.4. Metrics](#section54)\n",
    "    * [5.5. Compile and fit](#section55)\n",
    "* [6. Results](#section6)\n",
    "    * [6.1. Train/Validation representation](#section61)\n",
    "    * [6.2. Validation/Test evaluation](#section62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxTpMExHjpCa"
   },
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Context</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlVYt3MRrl_V"
   },
   "source": [
    "This is a tutorial on how to read the images created by TINTOlib and pass them to a Vision Transformer (ViT). The images must already be created by the TINTOlib software. See the documentation in GITHUB for how to create the images from tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RKBgDwzjpCl"
   },
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpU7pi6yjpCn"
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 1. Description</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL9RoFkEjpCq"
   },
   "source": [
    "The growing interest in the use of algorithms-based machine learning for predictive tasks has generated a large and diverse development of algorithms. However, it is widely known that not all of these algorithms are adapted to efficient solutions in certain tidy data format datasets. For this reason, novel techniques are currently being developed to convert tidy data into images with the aim of using Convolutional Neural Networks (CNNs) or Vision Transformer (ViT). TINTOlib offers the opportunity to convert tidy data into images through several techniques: TINTO, IGTD, REFINED, SuperTML, BarGraph, DistanceMatrix and Combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFrF4C89jpCt"
   },
   "source": [
    "---\n",
    "<a id=\"section11\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 1.1. Main Features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gruE0_sjpCu"
   },
   "source": [
    "- Supports all CSV data in **[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)** format.\n",
    "- For now, the algorithm converts tabular data for binary and multi-class classification problems into machine learning.\n",
    "- Input data formats:\n",
    "    - **Tabular files**: The input data could be in **[CSV](https://en.wikipedia.org/wiki/Comma-separated_values)**, taking into account the **[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)** format.\n",
    "    - **Dataframe***: The input data could be in **[Pandas Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)**, taking into account the **[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)** format.\n",
    "    - **Tidy Data**: The **target** (variable to be predicted) should be set as the last column of the dataset. Therefore, the first columns will be the features.\n",
    "    - All data must be in numerical form. TINTOlib does not accept data in string or any other non-numeric format.\n",
    "- Runs on **Linux**, **Windows** and **macOS** systems.\n",
    "- Compatible with **[Python](https://www.python.org/)** 3.7 or higher.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/Tabular-to-image-HyNNViT.jpg\" width=\"600\" height=\"320\" alt=\"Gr√°fica\">\n",
    "  <figcaption><blockquote>Vision Transformer (ViT) with synthetic images.</a></blockquote></figcaption>\n",
    "</center></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section12\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 1.2. Citation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TINTOlib** is an python library that makes **Synthetic Images** from [Tidy Data](https://www.jstatsoft.org/article/view/v059i10) (also knows as **Tabular Data**).\n",
    "\n",
    "**Citing TINTO**: If you used TINTO in your work, please cite the **[SoftwareX](https://doi.org/10.1016/j.softx.2023.101391)**:\n",
    "\n",
    "```bib\n",
    "@article{softwarex_TINTO,\n",
    "    title = {TINTO: Converting Tidy Data into Image for Classification\n",
    "            with 2-Dimensional Convolutional Neural Networks},\n",
    "    journal = {SoftwareX},\n",
    "    author = {Manuel Castillo-Cara and Reewos Talla-Chumpitaz and\n",
    "              Ra√∫l Garc√≠a-Castro and Luis Orozco-Barbosa},\n",
    "    year = {2023},\n",
    "    pages = {101391},\n",
    "    issn = {2352-7110},\n",
    "    doi = {https://doi.org/10.1016/j.softx.2023.101391}\n",
    "}\n",
    "```\n",
    "\n",
    "And use-case developed in **[INFFUS Paper](https://doi.org/10.1016/j.inffus.2022.10.011)**\n",
    "\n",
    "```bib\n",
    "@article{inffus_TINTO,\n",
    "    title = {A novel deep learning approach using blurring image\n",
    "            techniques for Bluetooth-based indoor localisation},\n",
    "    journal = {Information Fusion},\n",
    "    author = {Reewos Talla-Chumpitaz and Manuel Castillo-Cara and\n",
    "              Luis Orozco-Barbosa and Ra√∫l Garc√≠a-Castro},\n",
    "    volume = {91},\n",
    "    pages = {173-186},\n",
    "    year = {2023},\n",
    "    issn = {1566-2535},\n",
    "    doi = {https://doi.org/10.1016/j.inffus.2022.10.011}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section13\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 1.3. Documentation and License</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TINTOlib has a wide range of documentation on both GitHub and PiPY. \n",
    "\n",
    "Moreover, TINTOlib is free and open software with Apache 2.0 license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib in [GitHub](https://github.com/oeg-upm/TINTOlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib documentation in the official [Webpage](https://tintolib.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3tgsO0BjpCj"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib documentation in [PyPI](https://tintolib.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see more information and examples in [TINTOlib Crash Course](https://github.com/oeg-upm/TINTOlib-Crash_Course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3EzYcjJjpC6"
   },
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwYF5A2njpC8"
   },
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 2. Libraries</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section21\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 2.1. System setup</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before installing the libraries you must have the `mpi4py` package installed on the native (Linux) system. This link shows how to install it: \n",
    "- Link: [`mpi4py` in Linux](https://www.geeksforgeeks.org/how-to-install-python3-mpi4py-package-on-linux/)\n",
    "\n",
    "For example, in Linux:\n",
    "\n",
    "```\n",
    "    sudo apt-get install python3\n",
    "    sudo apt install python3-pip\n",
    "    sudo apt install python3-mpi4py\n",
    "```\n",
    "\n",
    "If you are in Windows, Mac or, also, Linux, you can install from PyPI if you want:\n",
    "```\n",
    "    sudo pip3 install mpi4py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Note that you must **restart the kernel or the system** so that it can load the libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once you have installed `mpi4py` you can install the PyPI libraries and dependences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchmetrics pytorch_lightning TINTOlib keras_preprocessing mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Note that you must **restart the kernel** so that it can load the libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section22\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 2.2. Invoke the libraries</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding parent directory to import module residing outside this notebook folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AdHKnWYsEq_"
   },
   "source": [
    "The first thing we need to do is to declare the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PeeBbGxlpjFp",
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtifffile\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtifi\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/pywrap_tensorflow.py:34\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m self_check\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mself_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreload_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m   \u001b[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the DLL(s) \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. TensorFlow requires that these DLLs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe installed in a directory that is named in your \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[38;5;241m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#import cv2\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "#import openslide\n",
    "#from openslide.deepzoom import DeepZoomGenerator\n",
    "import tifffile as tifi\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,mean_absolute_percentage_error\n",
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import vgg16, vgg19, resnet50, mobilenet, inception_resnet_v2, densenet, inception_v3, xception, nasnet, ResNet152V2\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, InputLayer, LayerNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adamax\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from tensorflow.keras.layers import Input, Activation,MaxPooling2D, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import ops\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#Models of TINTOlib\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination\n",
    "import TINTOlib.utils\n",
    "import random \n",
    "\n",
    "SEED = 64\n",
    "#torch.manual_seed(SEED)\n",
    "#torch.cuda.manual_seed(SEED)\n",
    "#torch.cuda.manual_seed_all(SEED)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwS-cKUxjpDQ"
   },
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDL4LARWjpDT"
   },
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 3. Data processing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXcRw78ljpDU"
   },
   "source": [
    "The first thing to do is to read all the images created by TINTO. TINTO creates a folder which contains subfolders corresponding to each target that has the problem. Each image corresponds to a sample of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section31\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 3.1. TINTOlib methods</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 3.1. Read the dataset</font>\n",
    "\n",
    "In this part, we proceed to read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cancer'\n",
    "\n",
    "#Read CSV\n",
    "df = pd.read_csv(f\"Datasets/{dataset_name}.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the appropriate size for a square image that contains all the feature pixels, you need to calculate the square root of the total number of features. The resulting value can be used for the methods that requires inserting the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delve into label encoding, a crucial preprocessing step for preparing categorical labels for use in machine learning models. Specifically for Binary classification tasks, label encoding involves transforming categorical labels into a binary format, represented by the integers 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the last column with LabelEncoder\n",
    "df.iloc[:, -1] = label_encoder.fit_transform(df.iloc[:, -1])\n",
    "\n",
    "# Display the updated last column\n",
    "print(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section32\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 3.2. Create images with TINTOlib</font>\n",
    "\n",
    "We prepare the declaration of the classes with the TINTOlib method we want to transform. Note that TINTOlib has several methods and we will have to choose one of them since each method generates different images.\n",
    "\n",
    "In addition, we establish the paths where the dataset is located and also the folder where the images will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "#problem_type = \"regression\"\n",
    "\n",
    "# Transformation methods\n",
    "#image_model = TINTO(problem=problem_type, blur=True, option='maximum', pixels=20, random_seed=SEED)\n",
    "#name = f\"TINTO_blur_maximum\"\n",
    "# image_model = REFINED(problem=problem_type, random_seed=SEED, zoom=1, n_processors=8)\n",
    "# name = f\"REFINED\"\n",
    "image_model = IGTD(problem=problem_type, scale=[image_size,image_size], zoom=3, fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=300, random_seed=SEED)\n",
    "name = f\"IGTD_fEuclidean_iEuclidean_abs\"\n",
    "# image_model = BarGraph(problem=problem_type, zoom=2)\n",
    "# name = f\"BarGraph_zoom2\"\n",
    "# image_model = DistanceMatrix(problem=problem_type, zoom=2)\n",
    "# name = f\"DistanceMatrix_zoom2\"\n",
    "# image_model = Combination(problem=problem_type, zoom=2)\n",
    "# name = f\"Combination_zoom2\"\n",
    "# image_model = SuperTML(problem=problem_type, pixels=pixel, font_size=30, feature_importance=True, random_seed=SEED)\n",
    "# name = f\"SuperTML-VF_FS30\"\n",
    "# image_model = FeatureWrap(problem = problem_type, bins=10)\n",
    "# name = f\"FeatureWrap_bins10\"\n",
    "# image_model = BIE(problem = problem_type)\n",
    "# name = f\"BIE\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "results_folder = f\"Results/CNN+MLP_Fusion/{dataset_name}_{name}\"\n",
    "images_folder = f\"Synthetic_images/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib documentation in [ReadTheDocs](https://tintolib.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section33\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 3.3. Generate images</font>\n",
    "\n",
    "In this section, we generate images from the dataset using three key functions of the image generation model:\n",
    "\n",
    "- fit: Trains the image generation model without generating images. This function is used exclusively for training purposes.\n",
    "- fit_transform: Trains the image generation model and simultaneously generates images for the dataset. This function is applied to the training dataset, where the model is both trained and used to create images.\n",
    "- transform: Generates images using the pre-trained model. After training on the training dataset, this function is used to generate images for unseen data, such as validation and test datasets.\n",
    "\n",
    "Each row in the dataset is transformed into a unique image, ensuring that the number of generated images matches the number of rows in the dataset. The resulting datasets include paths to these images, which are then combined with the original data for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_val = train_test_split(df, test_size=0.20, random_state=SEED)\n",
    "X_val, X_test = train_test_split(X_val, test_size=0.50, random_state=SEED)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to streamline the repetitive process of generating images, updating paths, and combining datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(X, folder_name, generate_function, problem_type):\n",
    "    \"\"\"\n",
    "    Handles dataset processing, including image generation, path updates, \n",
    "    and combining the dataset with image paths.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        The dataset to process (training, validation, or test).\n",
    "    \n",
    "    folder_name : str\n",
    "        The name of the folder where generated images will be stored \n",
    "        (e.g., 'train', 'val', 'test').\n",
    "    \n",
    "    generate_function : function\n",
    "        The function used for training and generating images. It can be one of the following:\n",
    "        - `fit`: Trains the model without generating images.\n",
    "        - `fit_transform`: Trains the model and generates images for the dataset (used for training).\n",
    "        - `transform`: Uses the pre-trained model to generate images for validation and testing.\n",
    "    \n",
    "    problem_type : str\n",
    "        The type of problem being addressed (e.g., regression, supervised).\n",
    "        This is used to locate the corresponding `.csv` file containing image paths.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_processed : DataFrame\n",
    "        The dataset with updated image paths and raw tabular data, ready for further processing.\n",
    "    \n",
    "    y_processed : Series\n",
    "        The labels corresponding to the dataset (target values).\n",
    "    \"\"\"\n",
    "    # Generate the images if the folder does not exist\n",
    "    folder_path = f\"{images_folder}/{folder_name}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        generate_function(X, folder_path)\n",
    "    else:\n",
    "        print(f\"The images for {folder_name} are already generated\")\n",
    "\n",
    "    # Load image paths\n",
    "    img_paths = os.path.join(folder_path, f\"{problem_type}.csv\")\n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = folder_path + \"/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, X], axis=1)\n",
    "\n",
    "    # Split data and labels\n",
    "    X_processed = combined_dataset.drop(df.columns[-1], axis=1).drop(\"class\", axis=1)\n",
    "    y_processed = combined_dataset[\"class\"]\n",
    "\n",
    "    return X_processed, y_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_train\n",
    "X_train, y_train = process_dataset(X_train, \"train\", image_model.fit_transform, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_val\n",
    "X_val, y_val = process_dataset(X_val, \"val\", image_model.transform, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_test\n",
    "X_test, y_test = process_dataset(X_test, \"test\", image_model.transform, problem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF1lJWbojpD3"
   },
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 4. Pre-modelling phase</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ready, we load it into memory with an iterator in order to pass it to the ViT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section41\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 4.1. Data curation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each method generates images of **different pixel size**. For example:\n",
    "- `TINTO` method has a parameter that you can specify the size in pixels which by default is 20.\n",
    "- Other parameters such as `Combined` generates the size automatically and you must obtain them from the _shape_ of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "You can see all information about TINTOlib documentation in [PyPI](https://tintolib.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "üí° **Important!!!**:  Keep in mind that, depending on the method used, you need to identify the number of pixels in the image. For example, in TINTO it is specified as a parameter, but in IGTD it is done afterwards, once the image is created (and even the pixels of width and height can be different)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIDY DATA SPLITTED\n",
    "X_train_num = X_train.drop(\"images\",axis=1)\n",
    "X_val_num = X_val.drop(\"images\",axis=1)\n",
    "X_test_num = X_test.drop(\"images\",axis=1)\n",
    "\n",
    "#IMAGES\n",
    "# For 3 channels (RGB)\n",
    "\"\"\"X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\"\"\"\n",
    "\n",
    "# For 1 channels (GRAY SCALE)\n",
    "X_train_img = np.array([cv2.imread(img,cv2.IMREAD_GRAYSCALE) for img in X_train[\"images\"]])\n",
    "X_val_img = np.array([cv2.imread(img,cv2.IMREAD_GRAYSCALE) for img in X_val[\"images\"]])\n",
    "X_test_img = np.array([cv2.imread(img,cv2.IMREAD_GRAYSCALE) for img in X_test[\"images\"]])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale numerical data\n",
    "X_train_num = scaler.fit_transform(X_train_num)\n",
    "X_val_num = scaler.transform(X_val_num)\n",
    "X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "attributes = X_train_num.shape[1]\n",
    "## For 3-channel\n",
    "#height, width, channels = X_train_img[0].shape\n",
    "## For 1-chanel\n",
    "height, width = X_train_img[0].shape\n",
    "channels = 1\n",
    "imgs_shape = (height, width, channels)\n",
    "\n",
    "print(\"Images shape: \",imgs_shape)\n",
    "print(\"Attributres: \",attributes)\n",
    "pixel=X_train_img[0].shape[0]\n",
    "print(\"Image size (pixels):\", pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_img[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example image (e.g., the first image in the array)\n",
    "example_image = X_train_img[0]\n",
    "\n",
    "# Convert the image from BGR (OpenCV default) to RGB for correct color display\n",
    "example_image_rgb = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(example_image_rgb)\n",
    "plt.title(\"Example Image from X_train\")\n",
    "plt.axis('off')  # Hide the axis for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 5. Modeling hybrid network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the ViT+MLP training. Before that we prepare the algorithm for reading data.\n",
    "\n",
    "In this example, 2 branch networks is created\n",
    "- 1¬∫ branch: FFNN for tabular data\n",
    "- 2¬∫ branch: ViT for TINTOlib images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section51\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.1. FFNN for tabular data</font>\n",
    "\n",
    "This is an example of a simple FFNN for tabular data. Note that we are not looking for the optimization of the ViT but to show an example of TINTOlib execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single branch FFNN\n",
    "ff_inputs = Input(shape = (attributes,))\n",
    "\n",
    "mlp_1 = Dense(64, activation='relu')(ff_inputs)\n",
    "mlp_1 = Dense(32, activation='relu')(mlp_1)\n",
    "mlp_1 = Dense(16, activation='relu')(mlp_1)\n",
    "ff_model = Model(inputs = ff_inputs, outputs = mlp_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section52\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.2. ViT for TINTOlib images</font>\n",
    "\n",
    "This is an example of a simple ViT for TINTOlib images. Note that we are not looking for the optimization of the ViT but to show an example of TINTOlib execution.\n",
    "\n",
    "It is crucial to select an appropriate patch size. The patch size should be a divisor of the input image size; for example, an image of 20x20 with a patch size of 5 would result in a total of 16 patches (4x4 grid). Given the high computational cost, the patch size should be carefully chosen based on the dimensions of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code helps identify the valid patch sizes by finding the divisors of a given number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors\n",
    "\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the hyperparameters for the ViT model implemented. The hyperparameters are as follows:\n",
    "\n",
    "- `image_size`: input image size.\n",
    "- `patch_size`: size of the patches extracted from the images.\n",
    "- `num_patches`: total number of patches extracted from each image.\n",
    "- `projection_dim`: dimensionality of the linear projection for the patches.\n",
    "- `num_heads`: number of attention heads in the transformer.\n",
    "- `transformer_units`: list of units in the transformer layers.\n",
    "- `transformer_layers`: number of layers in the transformer.\n",
    "- `mlp_head_units`: list of units in the dense layers of the final classifier.\n",
    "\n",
    "These hyperparameters are used to configure the ViT model and its training process.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/visionTransformer.png\" width=\"550\" height=\"350\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>ViT Architecture and components. Extracted from the <a href=\"https://arxiv.org/abs/2010.11929\">ViT article</a></blockquote></figcaption>\n",
    "</center></figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = pixel\n",
    "patch_size = 3\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 32\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 4\n",
    "mlp_head_units = [\n",
    "    128,\n",
    "    64,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section521\"></a>\n",
    "## <font color=\"#004D7F\" size=5> 5.2.1. `Patches` Class</font>\n",
    "\n",
    "The `Patches` class divides an image into small, fixed-size patches, rearranging them into a tensor that can be used as input for a Vision Transformer. This is essential because Transformers work with sequences, and this class converts images into sequences of patches.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/patch_embedding.png\" width=\"850\" height=\"200\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>ViT Architecture - Patch embedding. Extracted from <a href=\"https://arxiv.org/abs/2010.11929\">Aman Arora's Blog</a></blockquote></figcaption>\n",
    "</center></figure>\n",
    "\n",
    "The `Patches` class is a subclass of `layers.Layer` in Keras, meaning it is a custom layer. This class is responsible for dividing an image into small patches that will be used as inputs to the Vision Transformer.\n",
    "\n",
    "##### `__init__` Constructor\n",
    "- `__init__` is the class constructor.\n",
    "- `patch_size` is a parameter that specifies the size of each patch into which the image will be divided.\n",
    "- `super().__init__()` calls the constructor of the base class (`layers.Layer`), initializing the layer.\n",
    "\n",
    "##### `call` Method\n",
    "- `call` is the method that defines the logic of the layer. It is invoked when a tensor (in this case, images) is passed through the layer.\n",
    "- `input_shape = ops.shape(images)` retrieves the shape (dimensions) of the input tensor `images`. Assuming `images` is a 4D tensor (batch, height, width, channels).\n",
    "- `batch_size`, `height`, `width`, and `channels` extract the respective dimensions of the image.\n",
    "- `num_patches_h` and `num_patches_w` calculate the number of patches in the height and width of the image, respectively, by dividing the corresponding dimension by `patch_size`.\n",
    "- `patches = keras.ops.image.extract_patches(images, size=self.patch_size)` uses a Keras function to extract patches of size `patch_size` from the input images.\n",
    "- `patches = ops.reshape(...)` reshapes the `patches` tensor to have the shape `(batch_size, num_patches_h * num_patches_w, patch_size * patch_size * channels)`. This means that each patch is flattened and organized into a sequence of patches.\n",
    "\n",
    "##### `get_config` Method\n",
    "\n",
    "- `get_config` is a standard method in Keras for custom layers. It allows the layer's configuration to be saved and reloaded.\n",
    "- `config = super().get_config()` calls the base class's `get_config` method to retrieve the basic configuration of the layer.\n",
    "- `config.update({\"patch_size\": self.patch_size})` adds the `patch_size` to the configuration.\n",
    "- `return config` returns the complete configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = ops.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = ops.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section522\"></a>\n",
    "## <font color=\"#004D7F\" size=5> 5.2.2. Patch Encoder</font>\n",
    "\n",
    "The `PatchEncoder` class takes the image patches and projects them into a higher-dimensional space using a dense layer (`Dense`). It then adds positional information to each patch using a positional embedding layer (`Embedding`). This encoding is crucial for the functioning of Transformers, which need to know both the content of the patches and their position in the original image.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/vit.png\" width=\"450\" height=\"250\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>ViT Architecture and Transformer Encoder. Extracted from the <a href=\"https://arxiv.org/abs/2010.11929\">ViT article</a></blockquote></figcaption>\n",
    "</center></figure>\n",
    "\n",
    "The `PatchEncoder` class is a subclass of `layers.Layer` in Keras, and it is used to project and encode the image patches.\n",
    "\n",
    "##### `__init__` Constructor\n",
    "- `__init__` is the class constructor.\n",
    "- `num_patches` is the total number of patches into which the image has been divided.\n",
    "- `projection_dim` is the dimension into which the patches will be projected.\n",
    "- `self.projection` is a `Dense` layer that projects each patch into a higher-dimensional space specified by `projection_dim`.\n",
    "- `self.position_embedding` is an `Embedding` layer that creates positional embeddings for each patch, with `num_patches` as the vocabulary size and `projection_dim` as the output dimension.\n",
    "\n",
    "##### `call` Method\n",
    "- `call` is the method that defines the logic of the layer. It is invoked when a tensor (in this case, patches) is passed through the layer.\n",
    "- `positions` creates a tensor with a sequence of positions (from 0 to `num_patches - 1`), expanding the dimension to match the batch of data.\n",
    "- `projected_patches` applies a linear projection to each patch using the `Dense` layer, resulting in a higher-dimensional tensor.\n",
    "- `encoded` adds the linear projection of the patches (`projected_patches`) to the positional embeddings (`self.position_embedding(positions)`). This sum incorporates information about the position of each patch, which is crucial for the Transformer to understand the spatial arrangement of the patches.\n",
    "- `return encoded` returns the encoded tensor.\n",
    "\n",
    "##### `get_config` Method\n",
    "- `get_config` is a standard method in Keras for custom layers. It allows the layer's configuration to be saved and reloaded.\n",
    "- `config = super().get_config()` calls the base class's `get_config` method to retrieve the basic configuration of the layer.\n",
    "- `config.update({\"num_patches\": self.num_patches})` adds `num_patches` to the configuration.\n",
    "- `return config` returns the complete configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section523\"></a>\n",
    "## <font color=\"#004D7F\" size=5> 5.2.3. Classifier</font>\n",
    "\n",
    "The `mlp` function constructs an MLP that transforms the input tensor `x` through several dense and dropout layers. Each dense layer applies a linear transformation followed by a `gelu` activation, and each dropout layer randomly deactivates a percentage of the units from the previous layer to improve model generalization. This function is useful for adding non-linear learning capacity to the model, allowing it to capture more complex relationships in the data:\n",
    "- `x`: the input tensor to be transformed.\n",
    "- `hidden_units`: a list specifying the number of units (neurons) in each hidden layer of the MLP network.\n",
    "- `dropout_rate`: the dropout rate applied after each dense layer, helping to prevent overfitting.\n",
    "\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/vit.png\" width=\"450\" height=\"250\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>ViT Architecture and Transformer Encoder. Extracted from the <a href=\"https://arxiv.org/abs/2010.11929\">ViT article</a></blockquote></figcaption>\n",
    "</center></figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section524\"></a>\n",
    "## <font color=\"#004D7F\" size=5> 5.2.4. ViT Classifier</font>\n",
    "\n",
    "The `create_vit_classifier` function constructs a complete Vision Transformer classifier.\n",
    "1. It first divides the input image into patches and encodes those patches.\n",
    "2. Then, it passes the encoded patches through multiple Transformer layers, each of which includes layer normalization, multi-head attention, residual connections, and MLP layers.\n",
    "3. Finally, it normalizes and flattens the representation before passing it through a final MLP network to produce the output features.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/Encoder-decoder.png\" width=\"450\" height=\"250\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>Transformer Architecture. Extracted from the <a href=\"https://arxiv.org/abs/2010.11929\">ViT article</a></blockquote></figcaption>\n",
    "</center></figure>\n",
    "\n",
    "#### Model Input\n",
    "- `inputs`: defines the model input with a shape specified by `input_shape`.\n",
    "\n",
    "#### Patch Creation\n",
    "- `patches`: instantiates the `Patches` layer (defined earlier) to divide the input image into smaller patches of the size specified by `patch_size`.\n",
    "\n",
    "#### Patch Encoding\n",
    "- `encoded_patches`: instantiates the `PatchEncoder` layer (defined earlier) to project the patches into a high-dimensional space and add positional embeddings.\n",
    "\n",
    "#### Transformer Blocks\n",
    "- Loops through `transformer_layers` to create multiple layers of the Transformer block.\n",
    "    - `x1`: applies layer normalization to the `encoded_patches`.\n",
    "    - `attention_output`: applies a multi-head attention layer.\n",
    "    - `x2`: performs a residual (skip) connection by adding `attention_output` and `encoded_patches`.\n",
    "    - `x3`: applies another layer normalization to `x2`.\n",
    "    - `x3`: passes through an MLP using the `mlp` function defined earlier.\n",
    "    - `encoded_patches`: performs another residual connection by adding `x3` and `x2`.\n",
    "\n",
    "#### Final Representation\n",
    "- `representation`: applies layer normalization, flattens the tensor, and applies dropout for regularization.\n",
    "\n",
    "#### Final MLP Network\n",
    "- `features`: applies another MLP to the final representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = keras.Input(shape=imgs_shape)\n",
    "    # Augment data.\n",
    "    #augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=features)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import ops\n",
    "vit_model = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section53\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.3. Concatenate branches</font>\n",
    "\n",
    "Finally, we need to fuse the ViT and MLP branches. In this case, we use a direct concatenation of the output from the ViT branch with the output from the MLP branch, feeding them into a final fully connected neural network (FFNN) that will produce the predictions.\n",
    "\n",
    "<figure><center>\n",
    "  <img src=\"Images/Tabular-to-image-HyNNViT.jpg\" width=\"450\" height=\"250\" alt=\"Graph\">\n",
    "  <figcaption><blockquote>HyViT Architecture</blockquote></figcaption>\n",
    "</center></figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = concatenate([ff_model.output, vit_model.output])\n",
    "x = Dense(64, activation=\"relu\")(combinedInput)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=[ff_model.input, vit_model.input], outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section54\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.4. Metrics</font>\n",
    "\n",
    "Define metrics and some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    #tf.keras.metrics.TruePositives(name = 'tp'),\n",
    "    #tf.keras.metrics.FalsePositives(name = 'fp'),\n",
    "    #tf.keras.metrics.TrueNegatives(name = 'tn'),\n",
    "    #tf.keras.metrics.FalseNegatives(name = 'fn'), \n",
    "    tf.keras.metrics.BinaryAccuracy(name ='accuracy'),\n",
    "    tf.keras.metrics.Precision(name = 'precision'),\n",
    "    tf.keras.metrics.Recall(name = 'recall'),\n",
    "    tf.keras.metrics.AUC(name = 'auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section55\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.5. Compile and fit</font>\n",
    "\n",
    "Note to specify the **loss depending** on whether you have a binary or multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "opt = Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=opt,\n",
    "    metrics = METRICS\n",
    ")\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure EarlyStopping for binary classification\n",
    "early_stopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation accuracy\n",
    "    min_delta=0.001,         # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,             # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,               # Log when training stops\n",
    "    mode='min',              # Maximize the accuracy; min the loss\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history=model.fit(\n",
    "    x=[X_train_num, X_train_img], y=y_train,\n",
    "    validation_data=([X_val_num, X_val_img], y_val),\n",
    "    epochs=epochs , \n",
    "    batch_size=8,\n",
    "    callbacks = [early_stopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section6\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 6. Results</font>\n",
    "\n",
    "Finally, we can evaluate our hybrid model with the images created by TINTOlib in any of the ways represented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section61\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 6.1. Train/Validation representation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_history.history['loss'])\n",
    "plt.plot(model_history.history['loss'], color = 'red', label = 'loss')\n",
    "plt.plot(model_history.history['val_loss'], color = 'green', label = 'val loss')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['accuracy'], color = 'red', label = 'accuracy')\n",
    "plt.plot(model_history.history['val_accuracy'], color = 'green', label = 'val accuracy')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section62\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 6.2. Validation/Test evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test= model.evaluate([X_test_num, X_test_img], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict([X_test_num, X_test_img])\n",
    "real_values = y_test\n",
    "predicted_classes = (prediction > 0.5).astype(\"int\").reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = score_test[1]\n",
    "test_auc = score_test[4]\n",
    "test_precision = score_test[2]\n",
    "test_recall = score_test[3]\n",
    "\n",
    "print(\"Test accuracy:\",test_accuracy)\n",
    "print(\"Test AUC:\",test_auc)\n",
    "print(\"Test precision:\",test_precision)\n",
    "print(\"Test recall:\",test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = model_history.history[\"accuracy\"][-1]\n",
    "train_auc = model_history.history[\"auc\"][-1]\n",
    "train_precision = model_history.history[\"precision\"][-1]\n",
    "train_recall = model_history.history[\"recall\"][-1]\n",
    "train_loss = model_history.history[\"loss\"][-1]\n",
    "\n",
    "print(\"Train accuracy:\",train_accuracy)\n",
    "print(\"Train AUC:\",train_auc)\n",
    "print(\"Train precision:\",train_precision)\n",
    "print(\"Train recall:\",train_recall)\n",
    "print(\"Train loss:\",train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = model_history.history[\"val_accuracy\"][-1]\n",
    "validation_auc = model_history.history[\"val_auc\"][-1]\n",
    "validation_precision = model_history.history[\"val_precision\"][-1]\n",
    "validation_recall = model_history.history[\"val_recall\"][-1]\n",
    "validation_loss = model_history.history[\"val_loss\"][-1]\n",
    "\n",
    "print(\"Validation accuracy:\",validation_accuracy)\n",
    "print(\"Validation AUC:\",validation_auc)\n",
    "print(\"Validation precision:\",validation_precision)\n",
    "print(\"Validation recall:\",validation_recall)\n",
    "print(\"Validation loss:\",validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "true_classes = real_values\n",
    "\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix = cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = sklearn.metrics.classification_report(true_classes, \n",
    "                                               predicted_classes)\n",
    "print(report) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1plFq1CpEXIdc9LankaLPiOObRg0_y5l2",
     "timestamp": 1684250343977
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
