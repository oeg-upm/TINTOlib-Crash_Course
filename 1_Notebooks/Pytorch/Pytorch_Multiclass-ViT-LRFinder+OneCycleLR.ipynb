{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# einops library for tensor operations\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# Custom TINTO library imports\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination\n",
    "from TINTOlib.featureWrap import FeatureWrap\n",
    "from TINTOlib.bie import BIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"CUDA Version: {cuda_version}\")\n",
    "\n",
    "# Get cuDNN version\n",
    "cudnn_version = torch.backends.cudnn.version()\n",
    "print(f\"cuDNN Version: {cudnn_version}\")\n",
    "\n",
    "# Get PyTorch version\n",
    "pytorch_version = torch.__version__\n",
    "print(f\"PyTorch Version: {pytorch_version}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use GPU.\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Create a random tensor and move it to GPU to verify\n",
    "    x = torch.rand(5, 3)\n",
    "    print(f\"Is this tensor on GPU? {x.cuda().is_cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use CPU.\")\n",
    "\n",
    "# Additional check: is CUDA initialized?\n",
    "print(f\"Is CUDA initialized? {torch.cuda.is_initialized()}\")\n",
    "\n",
    "# Number of available GPUs\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Current device index\n",
    "print(f\"Current device index: {torch.cuda.current_device()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to store dataset name\n",
    "dataset_name = 'iris'\n",
    "results_path = f'./logs/Multiclass/{dataset_name}/ViT_Multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../../2_Data/Multiclass/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last column number of unique values\n",
    "n_classes = df.iloc[:,-1].nunique()\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=32):\n",
    "    \n",
    "    X_train, X_val = train_test_split(df, test_size=0.20, random_state=SEED)\n",
    "    X_val, X_test = train_test_split(X_val, test_size=0.50, random_state=SEED)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_val = X_val.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    \n",
    "    ### X_train\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/train'):\n",
    "        #Generate thet images\n",
    "        image_model.fit_transform(X_train, f'{images_folder}/train')\n",
    "        image_model.saveHyperparameters(f'{images_folder}'+\"/model.pkl\")\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(f'{images_folder}/train',problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "    \n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/train/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, X_train], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    X_train = combined_dataset.drop(df.columns[-1], axis=1).drop(\"class\", axis=1)\n",
    "    y_train = combined_dataset[\"class\"]\n",
    "        \n",
    "    ### X_val\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/val'):\n",
    "        #Generate thet images\n",
    "        image_model.transform(X_val, f'{images_folder}/val')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(f'{images_folder}/val',problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/val/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, X_val], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    X_val = combined_dataset.drop(df.columns[-1], axis=1).drop(\"class\", axis=1)\n",
    "    y_val = combined_dataset[\"class\"]\n",
    "    \n",
    "    ### X_test\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/test'):\n",
    "        #Generate thet images\n",
    "        image_model.transform(X_test, f'{images_folder}/test')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(f'{images_folder}/test',problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/test/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, X_test], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    X_test = combined_dataset.drop(df.columns[-1], axis=1).drop(\"class\", axis=1)\n",
    "    y_test = combined_dataset[\"class\"]\n",
    "    \n",
    "    # Numerical data\n",
    "    X_train_num = X_train.drop(\"images\", axis=1)\n",
    "    X_val_num = X_val.drop(\"images\", axis=1)\n",
    "    X_test_num = X_test.drop(\"images\", axis=1)\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\n",
    "\n",
    "    ## Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    ## Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val_num), columns=X_val_num.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    #ONE HOT ENCODING FOR MULTICLASS PROBLEMS\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    label_encoder.fit(y_train)\n",
    "\n",
    "    # Transform the target variable using one-hot encoding\n",
    "    y_train = label_encoder.transform(y_train)\n",
    "    y_val = label_encoder.transform(y_val)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.as_tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_val_img_tensor = torch.as_tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_test_img_tensor = torch.as_tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    y_train_tensor = torch.as_tensor(y_train, dtype=torch.long)\n",
    "    y_val_tensor = torch.as_tensor(y_val, dtype=torch.long)\n",
    "    y_test_tensor = torch.as_tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    X_train_img_tensor = X_train_img_tensor / 255.0\n",
    "    X_val_img_tensor = X_val_img_tensor / 255.0\n",
    "    X_test_img_tensor = X_test_img_tensor / 255.0\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, imgs_shape, patch_size):\n",
    "        super(Model1, self).__init__()\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape,\n",
    "            patch_size = patch_size,\n",
    "            dim = 32,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "            mlp_dim = 64,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, vit_input):\n",
    "        vit_output = self.vit(vit_input)\n",
    "        mlp_output = self.mlp(vit_output)\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, imgs_shape, patch_size):\n",
    "        super(Model2, self).__init__()\n",
    "        \n",
    "        # Enhanced ViT branch with increased depth and heads\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape,\n",
    "            patch_size = patch_size,\n",
    "            dim = 64,  # Increased dimensionality\n",
    "            depth = 4,  # More layers\n",
    "            heads = 8,  # More attention heads\n",
    "            mlp_dim = 128,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        # More complex MLP with Batch Normalization\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, vit_input):\n",
    "        vit_output = self.vit(vit_input)\n",
    "        mlp_output = self.mlp(vit_output)\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=100, min_lr=1e-3, max_lr=1, device='cuda', weight_decay=1e-2):\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=max_lr/min_lr, total_steps=total_steps, pct_start=0.3, final_div_factor=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    warm_up_epochs = epochs*0.3\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': [], 'train_f1': [], 'val_f1': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        for img_data, targets in train_loader:\n",
    "            img_data, targets = img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for img_data, targets in val_loader:\n",
    "                img_data, targets = img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "            #early_stopping_counter = 0\n",
    "        #else:\n",
    "            #if epoch > warm_up_epochs:\n",
    "                #early_stopping_counter += 1\n",
    "                #if early_stopping_counter >= early_stopping_patience:\n",
    "                    #print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    #break\n",
    "\n",
    "        train_accuracy = accuracy_score(train_targets, train_predictions)\n",
    "        train_f1 = f1_score(train_targets, train_predictions, average='weighted')\n",
    "        val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "        val_f1 = f1_score(val_targets, val_predictions, average='weighted')\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_accuracy': train_metrics['accuracy'],\n",
    "        'train_precision': train_metrics['precision'],\n",
    "        'train_recall': train_metrics['recall'],\n",
    "        'train_f1': train_metrics['f1'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_accuracy': val_metrics['accuracy'],\n",
    "        'val_precision': val_metrics['precision'],\n",
    "        'val_recall': val_metrics['recall'],\n",
    "        'val_f1': val_metrics['f1'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'test_precision': test_metrics['precision'],\n",
    "        'test_recall': test_metrics['recall'],\n",
    "        'test_f1': test_metrics['f1'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "    print(f\"Best Train Loss: {history['train_loss'][best_epoch-1]:.4f}, Best Val Loss: {history['val_loss'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train Accuracy: {history['train_accuracy'][best_epoch-1]:.4f}, Best Val Accuracy: {history['val_accuracy'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train F1: {history['train_f1'][best_epoch-1]:.4f}, Best Val F1: {history['val_f1'][best_epoch-1]:.4f}\")\n",
    "\n",
    "    # Save figures for this fold\n",
    "    os.makedirs(f\"models/Multiclass/{dataset_name}/ViT/{model_name}\", exist_ok=True)\n",
    "    plot_metric(history['train_loss'], history['val_loss'], 'Loss', dataset_name, model_name)\n",
    "    plot_metric(history['train_accuracy'], history['val_accuracy'], 'Accuracy', dataset_name, model_name)\n",
    "    plot_metric(history['train_f1'], history['val_f1'], 'F1 Score', dataset_name, model_name)\n",
    "    plot_learning_rate(history['learning_rate'], dataset_name, model_name)\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'logs/Multiclass/{dataset_name}/ViT/{model_name}', exist_ok=True)\n",
    "    with open(f'logs/Multiclass/{dataset_name}/ViT/{model_name}/metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "\n",
    "    # Save best model\n",
    "    model_save_path = f\"models/Multiclass/{dataset_name}/ViT/{model_name}/best_model.pth\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(best_model, model_save_path)\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics \n",
    "\n",
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_data, targets in data_loader:\n",
    "            img_data, targets = img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    plt.savefig(f\"models/Multiclass/{dataset_name}/ViT/{model_name}/{metric_name.lower()}_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_learning_rate(learning_rates, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    plt.savefig(f\"models/Multiclass/{dataset_name}/ViT/{model_name}/learning_rate_plot.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=64, epochs=100, min_lr=1e-3, max_lr=1 , device='cuda', weight_decay=1e-2):\n",
    "    try:\n",
    "        if model is None:\n",
    "            print(f\"Model {model_name} is None\")\n",
    "            return None\n",
    "        else:\n",
    "            # Compile and fit the model\n",
    "            metrics = compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, epochs=epochs, min_lr=min_lr, max_lr=max_lr, device=device, weight_decay=weight_decay)\n",
    "            return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compile and fit {model_name}: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clear CUDA cache and force garbage collection\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def try_create_model(model_class, patch_size, imgs_shape):\n",
    "    try:\n",
    "        model = model_class(imgs_shape[1:], patch_size)\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        sample_input = torch.randn(4, *imgs_shape)\n",
    "        output = model(sample_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "def run_lr_finder(model_class, patch_size, attributes, imgs_shape, dataset_name, name, train_loader, val_loader, num_iter):\n",
    "\n",
    "    # Define the path where the plot will be saved\n",
    "    save_dir = os.path.join(f\"logs/Multiclass/{dataset_name}/ViT/{name}\")\n",
    "    save_path = os.path.join(save_dir, 'lr_finder_plot.png')\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create and train Model\n",
    "        model = try_create_model(model_class, patch_size, imgs_shape)\n",
    "        \n",
    "        if model is None:\n",
    "            return None\n",
    "        \n",
    "        # Move model to device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-7, weight_decay=0.0001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "        lr_finder.range_test(train_loader, val_loader=val_loader, end_lr=1, num_iter=num_iter, step_mode=\"exp\")\n",
    "        \n",
    "        axis, lr = lr_finder.plot()\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Get the figure from the axis and save it\n",
    "        fig = axis.figure\n",
    "        fig.savefig(save_path)\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "        \n",
    "        # Close the figure to ensure it's saved properly\n",
    "        plt.close(fig)\n",
    "        \n",
    "        lr_finder.reset()\n",
    "        print(f\"Suggested learning rate: {lr}\")\n",
    "        \n",
    "        return lr\n",
    "    else:\n",
    "        print(f\"LR finder plot already exists at {save_path}. Skipping LR finder process.\")\n",
    "        # Load and display the existing image\n",
    "        img = plt.imread(save_path)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Turn off axis numbers and ticks\n",
    "        plt.title(\"Learning Rate Finder Plot\")\n",
    "        plt.show()\n",
    "        \n",
    "        return None  # Or you could return a default learning rate here\n",
    "\n",
    "# Usage example:\n",
    "# lr = run_lr_finder(Model1, attributes, imgs_shape, dataset_name, name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "#image_model = REFINED(problem= problem_type,hcIterations=5)\n",
    "#image_model = IGTD(problem= problem_type)\n",
    "#image_model = BarGraph(problem= problem_type)\n",
    "#image_model = DistanceMatrix(problem= problem_type)\n",
    "#image_model = Combination(problem= problem_type)\n",
    "#image_model = SuperTML(problem= problem_type)\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_TINTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iterations_per_epoch(dataset_size, batch_size):\n",
    "    iterations = dataset_size // batch_size\n",
    "    if dataset_size % batch_size != 0:\n",
    "        iterations += 1\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = calculate_iterations_per_epoch(df.shape[0], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 1: TINTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "name = f\"TINTO_blur\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1_patch2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-4, max_lr=4e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=2e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, option=\"maximum\", random_seed=SEED)\n",
    "name = f\"TINTO_blur_maximum\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=2e-4, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=8e-5, max_lr=4e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = TINTO(problem= problem_type, random_seed=SEED)\n",
    "name = f\"TINTO\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=3e-4, max_lr=6e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=3e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 2: IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=300, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}_fEuclidean_iEuclidean_abs\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1_patch1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1_patch1\", min_lr=1e-7, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=1e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=300, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size*2}x{image_size*2}_fEuclidean_iEuclidean_abs\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=5e-4, max_lr=7e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=2e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=300, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size*4}x{image_size*4}_fEuclidean_iEuclidean_abs\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=5e-5, max_lr=4e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=5e-5, max_lr=2e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=3e-4, max_lr=4e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=8e-5, max_lr=4e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], zoom=2, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size*2}x{image_size*2}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=4e-4, max_lr=8e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=4e-3)  # Train and evaluate Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], zoom=4, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size*4}x{image_size*4}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=5e-5, max_lr=5e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=5e-5, max_lr=2e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 3: REFINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = REFINED(problem= problem_type, random_seed=SEED)\n",
    "name = f\"REFINED\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-4, max_lr=2e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=2e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = REFINED(problem= problem_type, zoom=2, random_seed=SEED)\n",
    "name = f\"REFINED_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=8e-4)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=5e-6, max_lr=1e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = REFINED(problem= problem_type, zoom=4, random_seed=SEED)\n",
    "name = f\"REFINED_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=3e-6, max_lr=1e-4)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=6e-5)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 4: BAR GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = BarGraph(problem= problem_type)\n",
    "name = f\"BarGraph\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-7, max_lr=1e-4)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=5e-6, max_lr=5e-4)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = BarGraph(problem= problem_type, zoom=2)\n",
    "name = f\"BarGraph_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=2e-5, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=7e-4)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = BarGraph(problem= problem_type, zoom=4)\n",
    "name = f\"BarGraph_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=2e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=6e-6, max_lr=8e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 5: DISTANCE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = DistanceMatrix(problem= problem_type)\n",
    "name = f\"DistanceMatrix\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-7, max_lr=1e-1)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=3e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = DistanceMatrix(problem= problem_type, zoom=2)\n",
    "name = f\"DistanceMatrix_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=5e-4)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=1e-4)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = DistanceMatrix(problem= problem_type, zoom=4)\n",
    "name = f\"DistanceMatrix_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=5e-6, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=1e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 6: COMBINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = Combination(problem= problem_type)\n",
    "name = f\"Combination\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-7, max_lr=1e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=2e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = Combination(problem= problem_type, zoom=2)\n",
    "name = f\"Combination_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=2e-4, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=5e-5, max_lr=3e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = Combination(problem= problem_type, zoom=4)\n",
    "name = f\"Combination_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=5e-6,max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=2e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT 7: SUPERTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = SuperTML(problem= problem_type, random_seed=SEED)\n",
    "name = f\"SuperTML-EF\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=2e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=3e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = SuperTML(problem= problem_type, feature_importance=True, font_size=30, random_seed=SEED)\n",
    "name = f\"SuperTML-VF_FS30\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=2e-4, max_lr=1.5e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=2e-4, max_lr=1.5e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 8: FEATURE WRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = FeatureWrap(problem = problem_type)\n",
    "name = f\"FeatureWrap\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=3e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-6, max_lr=3e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 9: BINARY IMAGE ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = BIE(problem = problem_type)\n",
    "name = f\"BIE\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=3e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-6, max_lr=3e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## FINAL METRICS AND BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(base_path):\n",
    "    best_accuracy = float('-inf')\n",
    "    best_folder = None\n",
    "\n",
    "    # Walk through all directories and files in the base path\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == f'metrics.txt':\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read metrics from the file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    metrics = f.read()\n",
    "                \n",
    "                # Parse the metrics into a dictionary\n",
    "                metrics_dict = {}\n",
    "                for line in metrics.splitlines():\n",
    "                    key, value = line.split(': ')\n",
    "                    metrics_dict[key.strip()] = float(value.strip())\n",
    "                \n",
    "                # Check if the current folder has a better validation loss\n",
    "                if metrics_dict['test_accuracy'] > best_accuracy:\n",
    "                    best_accuracy = metrics_dict['test_accuracy']\n",
    "                    best_folder = root\n",
    "    \n",
    "    return best_folder, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_metrics(file_path):\n",
    "    metrics = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            key, value = line.split(': ')\n",
    "            metrics[key.strip()] = float(value.strip())\n",
    "    return metrics\n",
    "\n",
    "def rename_folder(old_folder_path, prefix):\n",
    "    folder_name = os.path.basename(old_folder_path)\n",
    "    new_folder_name = f\"{prefix}_{folder_name}\"\n",
    "    parent_dir = os.path.dirname(old_folder_path)\n",
    "    new_folder_path = os.path.join(parent_dir, new_folder_name)\n",
    "    os.rename(old_folder_path, new_folder_path)\n",
    "    return new_folder_path\n",
    "\n",
    "def process_folders(root_dir):\n",
    "    prefixes = [\"TINTO\", \"BarGraph\", \"Combination\", \"DistanceMatrix\", \"IGTD\", \"REFINED\", \"SuperTML\", \"FeatureWrap\", \"BIE\"]\n",
    "    best_folders = []\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        matching_folders = [f for f in os.listdir(root_dir) if f.startswith(prefix) and os.path.isdir(os.path.join(root_dir, f))]\n",
    "        if matching_folders:\n",
    "            best_folder = None\n",
    "            best_test_accuracy = float('-inf')\n",
    "            for folder in matching_folders:\n",
    "                metrics_file = os.path.join(root_dir, folder, 'metrics.txt')\n",
    "                if os.path.exists(metrics_file):\n",
    "                    metrics = read_metrics(metrics_file)\n",
    "                    if metrics['test_accuracy'] > best_test_accuracy:\n",
    "                        best_test_accuracy = metrics['test_accuracy']\n",
    "                        best_folder = folder\n",
    "            if best_folder:\n",
    "                new_path = rename_folder(os.path.join(root_dir, best_folder), \"TOP\")\n",
    "                best_folders.append(new_path)\n",
    "    \n",
    "    if best_folders:\n",
    "        overall_best_folder = None\n",
    "        overall_best_test_accuracy = float('-inf')\n",
    "        for folder in best_folders:\n",
    "            metrics_file = os.path.join(folder, 'metrics.txt')\n",
    "            if os.path.exists(metrics_file):\n",
    "                metrics = read_metrics(metrics_file)\n",
    "                if metrics['test_accuracy'] > overall_best_test_accuracy:\n",
    "                    overall_best_test_accuracy = metrics['test_accuracy']\n",
    "                    overall_best_folder = folder\n",
    "        if overall_best_folder:\n",
    "            rename_folder(overall_best_folder, \"BEST\")\n",
    "        \n",
    "    return best_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "base_path = f\"logs/Multiclass/{dataset_name}/ViT\"\n",
    "best_folders = process_folders(base_path)\n",
    "print(f\"Best model folder: {best_folders}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
